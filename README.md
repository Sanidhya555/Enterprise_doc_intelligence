# ğŸ§  Enterprise Document Intelligence System  
### Retrieval-Augmented Generation (RAG) with FAISS + FastAPI + Ollama

A production-ready enterprise document intelligence system that enables secure document ingestion, semantic search, and AI-powered question answering using Retrieval-Augmented Generation (RAG).

Built with FastAPI, FAISS, SentenceTransformers, and Ollama (Mistral), with a Streamlit frontend and Docker support.

---

## ğŸš€ Features

- âœ… Secure JWT Authentication
- âœ… PDF Document Ingestion
- âœ… Intelligent Text Chunking
- âœ… SentenceTransformer Embeddings
- âœ… FAISS Vector Search (Cosine Similarity)
- âœ… Retrieval-Augmented Generation (RAG)
- âœ… Ollama LLM Integration (Mistral)
- âœ… Streamlit Frontend UI
- âœ… Dockerized Deployment
- âœ… Production-Ready Project Structure

---

## ğŸ—ï¸ System Architecture

```text
User
  â†“
Streamlit Frontend
  â†“
FastAPI Backend
  â†“
Embedding Model (SentenceTransformers)
  â†“
FAISS Vector Store
  â†“
Top-K Retrieval
  â†“
Prompt Builder
  â†“
Ollama (Mistral LLM)
  â†“
Final Answer
```

---

## ğŸ“‚ Project Structure

```bash
enterprise_doc_intelligence/
â”‚
â”œâ”€â”€ app/                     # FastAPI application
â”‚   â”œâ”€â”€ api/                 # Route definitions
â”‚   â”œâ”€â”€ core/                # Config & security
â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ pipeline/                # RAG pipeline modules
â”‚   â”œâ”€â”€ chunking/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”œâ”€â”€ llm/
â”‚   â””â”€â”€ evaluation/
â”‚
â”œâ”€â”€ frontend/                # Streamlit UI
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ embeddings/          # FAISS index stored here (runtime generated)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| Backend | FastAPI |
| Frontend | Streamlit |
| Vector Database | FAISS (IndexFlatIP) |
| Embeddings | sentence-transformers |
| LLM | Ollama (Mistral) |
| Authentication | JWT |
| Deployment | Docker |
| Language | Python 3.10+ |

---

## ğŸ” Authentication

All protected endpoints require a valid JWT token.

### Login Endpoint


POST /login


Default credentials (configurable via `.env`):


username: admin
password: admin123


---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|------------|
| GET | /health | Health check |
| POST | /login | Generate JWT token |
| POST | /upload | Upload document |
| GET | /documents | List indexed documents |
| DELETE | /documents/{filename} | Delete document |
| POST | /query | Ask a question |
| GET | /metrics | System metrics |

---

## ğŸ§  RAG Workflow

1. Upload PDF document
2. Extract text
3. Chunk with overlap
4. Generate embeddings
5. Normalize embeddings
6. Store in FAISS (cosine similarity)
7. On query:
   - Embed query
   - Retrieve top-k chunks
   - Build contextual prompt
   - Send to Ollama
   - Return final answer

---

## ğŸ–¥ï¸ Local Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/Sanidhya555/Enterprise_doc_intelligence.git
cd Enterprise_doc_intelligence
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv .venv
.venv\Scripts\activate   # Windows
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Setup Environment Variables

Create a `.env` file in the root directory:

```env
SECRET_KEY=your_secret_key
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin123
OLLAMA_BASE_URL=http://localhost:11434
```

---

### 5ï¸âƒ£ Start Ollama

Install Ollama from:

https://ollama.com/download

Then run:

```bash
ollama serve
ollama pull mistral
```

---

### 6ï¸âƒ£ Run Backend

```bash
uvicorn app.main:app --reload
```

Swagger Docs:  
http://127.0.0.1:8000/docs

---

### 7ï¸âƒ£ Run Frontend

```bash
streamlit run frontend/app.py
```

Open:  
http://localhost:8501


## ğŸ³ Docker Setup

### Build Image

```bash
docker build -t enterprise-rag -f docker/Dockerfile .
```

### Run Container

```bash
docker run -p 8000:8000 enterprise-rag
```
---

## ğŸ“Š FAISS Configuration

- **Index Type:** `IndexFlatIP`
- **Similarity Metric:** Cosine similarity (L2-normalized vectors)
- **Embedding dtype:** `float32`
- **Persistent Storage:** `data/embeddings/`

---

## ğŸ”’ Production Considerations

- Runtime-generated FAISS index is not committed to Git
- Sensitive configuration values stored in `.env`
- Vector normalization ensures consistent cosine similarity scoring
- Clean repository structure (no virtual environment or generated files)
- Modular RAG pipeline for maintainability and scalability

---

## ğŸš€ Future Improvements

- Role-Based Access Control (RBAC)
- Streaming LLM responses
- Async embedding pipeline
- Scalable FAISS IVF index
- Cloud storage integration (S3)
- CI/CD pipeline
- Kubernetes deployment
- Redis caching layer

---

## ğŸ¯ Use Cases

- Enterprise document search
- Internal knowledge assistant
- Legal document analysis
- HR policy Q&A
- Research document summarization

---

## ğŸ‘¨â€ğŸ’» Author

**Sanidhya Sachin Kulkarni**  
AI/ML Engineer | Backend Developer  

---

## ğŸ“œ License

MIT License